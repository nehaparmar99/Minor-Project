<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>HOME</title>
    <link
      rel="stylesheet"
      href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"
    />
    <link rel="stylesheet" href="style.css" />
    <script src="https://unpkg.com/peerjs@1.3.1/dist/peerjs.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/peerjs/1.3.1/peerjs.min.js.map"></script>
    <script src="/socket.io/socket.io.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@2.4.0/dist/tf.min.js"></script>
    <script
      src="https://cdn.jsdelivr.net/npm/@tensorflow-models/face-landmarks-detection@0.0.1/dist/face-landmarks-detection.js"></script>
  <script>
      const ROOM_ID = "<%= roomId %>";
    </script>
  </head>
  <body>
    <div class="main">
      <div class="main__left" id = "videoroom">
        <div class="main__videos">
          <div id="video-grid"></div>
        </div>
<canvas id="output" class="local-video"></canvas>
        <div class="main__controls">
          <div class="main__controls_block">
            <div
              class="main__controls_button"
              id="muteButton"
              onclick="muteUnmute()"
            >
              <i class="fa fa-microphone"></i>
              <span>Mute</span>
            </div>
            <div
              class="main__controls_button"
              id="playPauseVideo"
              onclick="playStop()"
            >
              <i class="fa fa-video-camera"></i>
              <span>Pause Video</span>
            </div>
          </div>

          <div class="main__controls_block">
            <div class="main__controls_button" id = "securitybutton" onclick = "myconsole()">
              <i class="fa fa-shield"></i>
              <span>Security</span>
            </div>
            <div class="main__controls_button">
              <i class="fa fa-users"></i>
              <span>Participants</span>
            </div>
            <div class="main__controls_button" onclick = "togglechatbox()">
              <i class="fa fa-comment"></i>
              <span>Chat</span>
            </div>
          </div>

          <div class="main__controls_block">
            <div class="main__controls_button leaveMeeting" id="leave-meeting" onclick = "leavemeeting()">
              <i class="fa fa-times"></i>
              <span class="">Leave Meeting</span>
            </div>
          </div>
        </div>
      </div>
      <div class="main__right" id = "chatroom">
        <canvas width = "600px" height = "400px" id = "tmpcanvas"></canvas>
        <img alt="Italian Trulli" id = "tmpimage">
        <button id = "capturebutton" onclick = "capture()">Capture</button>
        <div class="main__header">
          <h6>Chat</h6>
        </div>
        <div class="main__chat__window" id="main__chat__window">
          <ul class="messages" id="all_messages"></ul>
        </div>
        <div class="main__message_container">
          <input
            type="text"
            id="chat_message"
            placeholder="Type message here.."
          />
        </div>
      </div>
    </div>
  <script>
        function drawLine(ctx, x1, y1, x2, y2) {
          ctx.beginPath();
          ctx.moveTo(x1, y1);
          ctx.lineTo(x2, y2);
          ctx.stroke();
        }
        const emotions = [
          "angry",
          "disgust",
          "fear",
          "happy",
          "neutral",
          "sad",
          "surprise",
        ];
        let emotionModel = null;
        let output = null;
        let model = null;
    let emomodel;
        async function predictEmotion(points) {
          let result = tf.tidy(() => {
          // xs= tf.expand_dims(img,axis=0)
        const prediction = emomodel.predict(tf.reshape(points, shape = [138, 48, 48, 1]));// const xs = tf.stack([tf.tensor1d(points)]);
            // return emomodel.predict(xs);
          console.log(prediction);
          return prediction;
          });
          let prediction = await result.data();
          result.dispose();
          // Get the index of the maximum value
          let id = prediction.indexOf(Math.max(...prediction));
          console.log(emotions[id])
          return emotions[id];
        }
        async function trackFace() {
          const video = document.querySelector("video");
         // CAPTURE FACE EVERY 5 min(Might use setInterval) and send the image to /preprocess for points

          const faces = await model.estimateFaces({
            input: video,
            returnTensors: false,
            flipHorizontal: false,
          });
         console.log(faces);
          let points = null;
          faces.forEach((face) => {
            // Draw the bounding box
            const x1 = face.boundingBox.topLeft[0];
            const y1 = face.boundingBox.topLeft[1];
            const x2 = face.boundingBox.bottomRight[0];
            const y2 = face.boundingBox.bottomRight[1];
            const bWidth = x2 - x1;
            const bHeight = y2 - y1;
console.log("in track")
            // Add just the nose, cheeks, eyes, eyebrows & mouth
            const features = [
              "noseTip",
              "leftCheek",
              "rightCheek",
              "leftEyeLower1",
              "leftEyeUpper1",
              "rightEyeLower1",
              "rightEyeUpper1",
              "leftEyebrowLower", //"leftEyebrowUpper",
              "rightEyebrowLower", //"rightEyebrowUpper",
              "lipsLowerInner", //"lipsLowerOuter",
              "lipsUpperInner", //"lipsUpperOuter",
            ];
            points = [];
            features.forEach((feature) => {
              face.annotations[feature].forEach((x) => {
                points.push((x[0] - x1) / bWidth);
                points.push((x[1] - y1) / bHeight);
              });
            });
          });
          if(points)
          console.log(points)

//           if (points) {
//            // let emotion = await predictEmotion(points);
//             // setText(`Detected: ${emotion}`);
//              console.log(points)
//             //  fetch("http://127.0.0.1:5000/scale",{
//             //    method: "POST",
//             //     headers: {
//             //       Accept: "application/json,*/*",
//             //       "Content-Type": "application/json",
//             //     },
//             //     body: JSON.stringify({
//             //       points: points,
//             //     }),
//             //  }).then((res) => res.json())
//             //     .then(res => console.log(res))
//             //     .catch((err) => console.log(err));
  
// //             setInterval(function(){
// //               fetch("http://127.0.0.1:3030/predict", {
// //                 method: "POST",
// //                 headers: {
// //                   Accept: "application/json,*/*",
// //                   "Content-Type": "application/json",
// //                 },
// //                 body: JSON.stringify({
// //                   points: points,
// //                 }),
// //               })
// //                 .then((res) => res.json())
// //                 .then(res => console.log(res))
// //                 .catch((err) => console.log(err));
// //             }
// // ,6000
// //             )
//           } else {
//             setText("No Face");
//           }

          requestAnimationFrame(trackFace);
        }

        (async () => {
          const video = document.querySelector("video");
          const modelURL = 'http://127.0.0.1:5000/model';
      
          // Load Face Landmarks Detection
          model = await faceLandmarksDetection.load(
            faceLandmarksDetection.SupportedPackages.mediapipeFacemesh
          );
          if (!emomodel)
            emomodel = await tf.loadLayersModel(modelURL);
          trackFace();
        })();
  </script>
    <script src="script.js"></script>
  </body>
</html>
